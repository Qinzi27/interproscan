data.directory=data
bin.directory=bin

# Temporary files used by the analyses will be placed in directories here
# (the text [UNIQUE], if present, will be replaced by a value unique to your running instance)
temporary.file.directory.suffix=[UNIQUE]
temporary.file.directory=temp/${temporary.file.directory.suffix}

# Instructs I5 to completely clean up after a run
delete.temporary.directory.on.completion=true

# Perl and Python binaries to use
perl.command=perl
python3.command=python3

# Binary file locations
#rpsblast
binary.rpsblast.path=${bin.directory}/blast/rpsblast

#rpsbproc
binary.rpsbproc.path=${bin.directory}/blast/rpsbproc

#hmmer 3
binary.hmmer3.path=${bin.directory}/hmmer/hmmer3/3.1b1
binary.hmmer3.hmmscan.path=${bin.directory}/hmmer/hmmer3/3.1b1/hmmscan
binary.hmmer3.hmmsearch.path=${bin.directory}/hmmer/hmmer3/3.1b1/hmmsearch

binary.hmmer33.path=${bin.directory}/hmmer/hmmer3/3.3
binary.hmmer33.hmmscan.path=${bin.directory}/hmmer/hmmer3/3.3/hmmscan
binary.hmmer33.hmmsearch.path=${bin.directory}/hmmer/hmmer3/3.3/hmmsearch

#hmmer 2
binary.hmmer2.hmmsearch.path=${bin.directory}/hmmer/hmmer2/2.3.2/hmmsearch
binary.hmmer2.hmmpfam.path=${bin.directory}/hmmer/hmmer2/2.3.2/hmmpfam
binary.fingerprintscan.path=${bin.directory}/prints/fingerPRINTScan
binary.coils.path=${bin.directory}/ncoils/2.2.1/ncoils


# Note: Correct prosite binary distribution for your platform can be downloaded: ftp://ftp.expasy.org/databases/prosite/ps_scan/
binary.prosite.psscan.pl.path=${bin.directory}/prosite/ps_scan.pl
binary.prosite.pfscan.path=${bin.directory}/prosite/pfscan
binary.prosite.pfsearch.path=${bin.directory}/prosite/pfsearch
binary.prosite.pfscanv3.path=${bin.directory}/prosite/pfscanV3
binary.prosite.pfsearchv3.path=${bin.directory}/prosite/pfsearchV3
binary.prosite.pfsearch.wrapperpath=${bin.directory}/prosite/pfsearch_wrapper.py
binary.prosite.runprosite.path=${bin.directory}/prosite/runprosite.py

# FunFam
binary.funfam.path=${bin.directory}/funfam/4.3.0/search.py

#CATH-Gene3d
cath.resolve.hits.path=${bin.directory}/gene3d/4.3.0/cath-resolve-hits
assign.cath.superfamilies.path=${bin.directory}/gene3d/4.3.0/assign_cath_superfamilies.py

#PANTHER
binary.epang.path=${bin.directory}/panther/epa-ng
binary.treegrafter.path=${bin.directory}/panther/treegrafter.py

#PIRSF
binary.pirsf.pl.path=${bin.directory}/pirsf/3.10/pirsf.pl

binary.getorf.path=${bin.directory}/nucleotide/getorf
binary.esltranslate.path=${bin.directory}/nucleotide/esl-translate

#PIRSR
pirsr.postprocess.command=${bin.directory}/pirsr/pirsr_postprocess

#SFLD
sfld.postprocess.command=${bin.directory}/sfld/sfld_postprocess

#SUPERFAMILY
binary.superfamily.1.75.ass3.pl.path=${bin.directory}/superfamily/1.75/ass3_single_threaded.pl

#signalp
# Note: SignalP binary not distributed with InterProScan 5, please install separately e.g. in ${bin.directory}/signalp/4.1/signalp
binary.signalp.path=${bin.directory}/signalp/4.1/signalp
signalp.perl.library.dir=${bin.directory}/signalp/4.1/lib

#TMHMM 2.0
# Note: TMHMM binary not distributed with InterProScan 5, please install separately e.g. in ${bin.directory}/tmhmm/2.0c/decodeanhmm
binary.tmhmm.path=${bin.directory}/tmhmm/2.0c/decodeanhmm

#PHOBIUS
# Note: Phobius binary not distributed with InterProScan 5, please install separately e.g. in ${bin.directory}/phobius/1.01/phobius.pl
binary.phobius.pl.path=${bin.directory}/phobius/1.01/phobius.pl


####################################
# Member databases HMMs
####################################
antifam.hmm.path=${data.directory}/antifam/7.0/AntiFam.hmm
gene3d.hmm.path=${data.directory}/gene3d/4.3.0/gene3d_main.hmm
hamap.hmm.path=${data.directory}/hamap/2021_04/hamap.hmm.lib
panther.hmm.path=${data.directory}/panther/17.0/famhmm/binHmm
pfam-a.hmm.path=${data.directory}/pfam/35.0/pfam_a.hmm
pirsf.sfhmm.path=${data.directory}/pirsf/3.10/sf_hmm_all
pirsr.srhmm.path=${data.directory}/pirsr/2021_05/sr_hmm_all
sfld.hmm.path=${data.directory}/sfld/4/sfld.hmm
superfamily.hmm.path=${data.directory}/superfamily/1.75/hmmlib_1.75
tigrfam.hmm.path=${data.directory}/tigrfam/15.0/TIGRFAMs_HMM.LIB

# A list of TCP ports that should not be used for messaging
# (Apart from this, only ports > 1024 and < 65535 will be used)
tcp.port.exclusion.list=3879,3878,3881,3882

####################################
# Precalculated match lookup service
####################################

# By default, if the sequence already has matches available from the EBI, this service will look them
# up for you.  Note: at present it will always return all the available matches, ignoring any -appl options
# set on the command line
precalculated.match.lookup.service.url=https://www.ebi.ac.uk/interpro/match-lookup
precalculated.match.protein.lookup.batch.size=100
precalculated.match.protein.insert.batch.size=500
precalculated.match.protein.insert.batch.size.nolookup=4000

# Proxy set up
precalculated.match.lookup.service.proxy.host=
precalculated.match.lookup.service.proxy.port=3128

# Exclude sites from output (residue level annotations)
exclude.sites.from.output=false


####################################
# Master/Standalone embedded workers
####################################

# Set the number of embedded workers to the number of processors that you would like to employ
# on the machine you are using to run InterProScan.
number.of.embedded.workers=6
maxnumber.of.embedded.workers=8

#################################
# Distributed mode (Cluster mode)
#################################

# Grid name
grid.name=lsf
#grid.name=other-cluster

# Project name for this run
user.digest=i5GridRun

# Grid jobs limit : number of jobs you are allowed to run on the cluster
grid.jobs.limit=1000

# Time between each bjobs/qstat command to check the status of jobs on the cluster
grid.check.interval.seconds=120

# Allow master interproscan to run binaries
master.can.run.binaries=true

# Deal with unknown step states
recover.unknown.step.state=false

# Grid submission commands (e.g. LSF bsub or SGE qsub) for starting remote workers
# Commands the master uses to start new remote workers
grid.master.submit.command=bsub -q QUEUE_NAME -M 8192
grid.master.submit.high.memory.command=bsub -q QUEUE_NAME -M 8192

# Commands a worker uses to start new remote workers
grid.worker.submit.command=bsub -q QUEUE_NAME -M 8192
grid.worker.submit.high.memory.command=bsub -q QUEUE_NAME -M 8192

# Command to start a new worker (new jvm)
worker.command=java -Xms2028M -Xmx9216M -jar interproscan-5.jar
# This may be identical to the worker.command argument above, however you may choose to select
# a machine with a much larger available memory, for use when a StepExecution fails.
worker.high.memory.command=java -Xms2028M -Xmx9216M -jar interproscan-5.jar

# Set the number of embedded workers to the number of processors that you would like to employ
# on the node machine on which the worker will run.
worker.number.of.embedded.workers=4
worker.maxnumber.of.embedded.workers=4

# Max number of connections to the master
master.maxconsumers=48

# Number of connections to the worker
worker.maxconsumers=32

# Throttled network?
grid.throttle=true

# Max number of jobs a tier 1 worker is allowed on its queue
worker.maxunfinished.jobs=32

# Network tier depth
max.tier.depth=1

# Active MQ JMS broker temporary data directory
jms.broker.temp.directory=activemq-data/localhost/tmp_storage
